# Meeting 16.11.2022 - Progress

## 1. Bisheriger Fortschritt

### Experimentelle Umsetzung - Datenbankstrukturen:
Bisher experimentell umgesetzt:

- **knowledgegraph**(sub, pre, object) - Mit Covering Clustered Primary Index auf (subject, predicate, object) 
  - In der Datenbank sind Table und Attribute in Abkürzungen geschrieben, um die Query größe zu vermindern
- Für jede existierende Relation wurde eine materialized View v0, v1,...vN mit N = Anzahl existierender Relationen erstellt. Jede View hat die Form v(subject, object) mit Unique Clustering Index auf (subject, object)
  - Für jedes Triple im Body der Regel die validiert werden soll, wird anstatt im Knowledgegraph jetzt in der entsprechenden View die für das Body Tripel passt, gesucht
  - Ansatz ist hier, dass vor allem bei sehr großen Knowledgegraphen und vielen Regeln in einer Query die Suchzeit im B-Tree der Queries zu hoch wird
  - Daher wird der knowledgegraph in kleinere Teile aufgeteilt aufgeteilt
  - **Nachteil:** Vergleiche zwischen Bodys finden dadurch über verschiedene Tables hinweg statt (Hypothese ist daher, dass es nur für große Regelmengen relevant wird)

Andere Ansätze wie Indizes nur auf zwei der 3 Spalten oder gar keine Indizes haben in ersten Tests schlechter abgeschnitten. Hier haben wir aber noch keine vergleichbaren Daten, da wir es noch nicht auf dem Server testen konnten.

### Wie funktioniert eine Query:

- Eine Query hat die Form:
  - (SELECT {RuleID} FROM iku k1, iku k2, ... WHERE k1.pre = ... AND k2.pre = ... AND k1.sub = ... AND ... LIMIT 1) UNION ALL SELECT {RuleID2} FROM ...
    - Statement wurde umgeschrieben in SELECT ... LIMIT 1


### Messungen

- Knowledgegraph: Trainingsset YAGO3 - 1079040 Tripel
- Regelmenge: AnyBURL generierte Regelmenge nach 50s
    - Gefiltert nach simplen Regeln (Da AnyBURL so auch vorgeht):
        1. h(X, Y ) ← r(X, Y )
        2. h(X, Y ) ← r1(X, Z) ∧ r2(Z, Y ) (2)
        3. h(X, e1) ← r(X, e2), (3)
    - Regelmenge von **19271** simplen Regeln
- Query Menge: Validierungsdatenset YAGO3 - 5000 Tripel
- Messung der Gesamtdauer
- Messung der Durchschnittszeit pro Anfrage
- Die Messungen haben wir auf dem Uni Server durchgeführt:
  - 

NoViews:
Gesamtzeit: 18365 ms
Durchschnittszeit: 3 ms
Abfragen: 5000

Views:
Gesamtzeit: 27384 ms
Durchschnittszeit: 5 ms
Abfragen: 5000


**Durchschnittszeit / Gesamtzeit der Anfragen**
|                |               | Messung      | 
| -------------- | ------------- | ------------ |
| AnyBURL        | Durchschnitt  | 11 ms        |
|                | Gesamt        | 55982 ms     |
| View Umsetzung | Durchschnitt  | 5 ms         | 
|                | Gesamt        | 27384        | 
| Indexed Kg     | Durchschnit   | 3 ms         | 
|                | Gesamt        | 18365 ms     | 

## 2. Next Steps

**1. Paper Voranbringen**
- Vorgehensweise, Datenbankstruktur, Methoden und Messungsergebnisse in Paper einarbeiten
- Allgemein fokus hierauf legen

**2. Weitere Messungen Durchführen**
  - Bereits vollständig programmiert, nur noch nicht getestet:
    - Umsetzung mit verschiedenen Indizierungsmethoden (z.B. nur teilweise indexierung der Spalten, je nachdem was gebraucht wird)
    - Messen der nativen Methode ohne Indizes
    - Messung mit sehr großer Regelmenge (z.B nicht mehr filtern nach simplen Regeln) ?
  - Noch nicht vollständig programmiert:
    - Aufteilen der sehr großen SQL Statements in verschiedene kleinere, die parallel per Threading über an die Datenbank gesendet werden (über mehrere connections)
    - Vollständige Umsetzung in der Datenbank, inklusive der Regeln (wahrscheinlich nicht effizienter als bisherige Methoden)
      - Regeln ebenfalls Teil der Datenbank statt Klassenstruktur

## Fragen
  - Nur One Step reasoning oder vollständiges Reasoning oder beides? (Bisher nur one-step reasoning)
  - Laut AnyBURL Paper erkenntnissen sind nur kurze Regeln gut in Embedding Models repräsentiert
    - Sollen wir uns nur auf die Vergleichbarkeit mit AnyBURL und der Code Umsetzung fokussieren oder auch Messungen für größere Regelmengen machen?

