# Meeting 16.11.2022 - Progress

## 1. Bisheriger Fortschritt

### Experimentelle Umsetzung - Datenbankstrukturen:
Bisher experimentell umgesetzt:

- **Knowledgegraph**(sub, pre, object) - Mit Covering Clustered Primary Index auf (subject, predicate, object) 
  - In der Datenbank sind Table und Attribute in Abkürzungen geschrieben, um die Query größe zu vermindern
- Für jede existierende Relation wurde eine materialized View v0, v1,...vN mit N = Anzahl existierender Relationen erstellt. Jede View hat die Form v(subject, object) mit Unique Clustering Index auf (subject, object)
  - Für jedes Triple im Body der Regel die validiert werden soll, wird anstatt im Knowledgegraph jetzt in der entsprechenden View die für das Body Tripel passt, gesucht
  - Ansatz ist hier, dass vor allem bei sehr großen Knowledgegraphen und vielen Regeln in einer Query die Suchzeit im B-Tree der Queries zu hoch wird
  - Daher wird der Knowledgegraph in kleinere Teile aufgeteilt.
  - **Nachteil:** Vergleiche zwischen Bodys finden dadurch über verschiedene Tables hinweg statt (Hypothese ist daher, dass es nur für große Regelmengen relevant wird)

Andere Ansätze wie Indizes nur auf zwei der 3 Spalten oder gar keine Indizes haben in ersten Tests schlechter abgeschnitten. Hier haben wir aber noch keine vergleichbaren Daten, da wir es noch nicht auf dem Server testen konnten.

### Wie funktioniert eine Query:

- Regeln werden vorpartitioniert:
    - Eine Frei, eine gebunden
    - Eine gebunden, eine frei
    - Beide gebunden
    - Beide Frei

- Eine Query hat die Form:
  - (SELECT {RuleID} FROM iku k1, iku k2, ... WHERE k1.pre = ... AND k2.pre = ... AND k1.sub = ... AND ... LIMIT 1) UNION ALL SELECT {RuleID2} FROM ...
    - Statement wurde umgeschrieben in SELECT ... LIMIT 1


### Messungen

- Knowledgegraph: Trainingsset YAGO3 - 1079040 Tripel
- Regelmenge: AnyBURL generierte Regelmenge nach 50s
    - Gefiltert nach simplen Regeln (Da AnyBURL so auch vorgeht):
        1. h(X, Y ) ← r(X, Y )
        2. h(X, Y ) ← r1(X, Z) ∧ r2(Z, Y ) (2)
        3. h(X, e1) ← r(X, e2), (3)
    - Alle Regeln
- Query Menge: Validierungs-Datenset YAGO3 - 5000 Tripel
- Messung der Gesamtdauer
- Messung der Durchschnittszeit pro Anfrage
- Die Messungen haben wir auf dem BW-PC der Uni durchgeführt:
  - Fujitsu Esprimo P957 - Baujahr 2017
  - CPU = Intel i7-7700 @ 3.6 GHz
  - Arbeitsspeicher = 32 GB
  - SSD = 512 GB
  - HDD = 2 TB
  - Betriebssystem = Ubuntu 22.04.1 LTS

**Durchschnittszeit / Gesamtzeit der Anfragen**
|                |               | Gefilterte Regeln (19271 Regeln) | Alle Regeln (106480 Regeln) | 
| -------------- | ------------- | ------------ | ------------ |
| AnyBURL        | Durchschnitt  | 11 ms        | -            |
|                | Gesamt        | 55982 ms     | -            |
| View Umsetzung | Durchschnitt  | 5 ms         | 194 ms       | 
|                | Gesamt        | 27384 ms     | 973543 m     | 
| Indexed Kg     | Durchschnit   | 3 ms         | - ms         | 
|                | Gesamt        | 18365 ms     | - ms         | 

## 2. Next Steps

**1. Paper Voranbringen**
- Vorgehensweise, Datenbankstruktur, Methoden und Messungsergebnisse in Paper einarbeiten
- Allgemein fokus hierauf legen

**2. Weitere Messungen Durchführen**
  - Bereits vollständig programmiert, nur noch nicht getestet:
    - Umsetzung mit verschiedenen Indizierungsmethoden (z.B. nur teilweise indexierung der Spalten, je nachdem was gebraucht wird)
    - Messen der nativen Methode ohne Indizes
    - Messung mit sehr großer Regelmenge (z.B nicht mehr filtern nach simplen Regeln) ?
  - Noch nicht vollständig programmiert:
    - Aufteilen der sehr großen SQL Statements in verschiedene kleinere, die parallel per Threading über an die Datenbank gesendet werden (über mehrere connections)
    - Vollständige Umsetzung in der Datenbank, inklusive der Regeln (wahrscheinlich nicht effizienter als bisherige Methoden)
      - Regeln ebenfalls Teil der Datenbank statt Klassenstruktur

  Vorgehensweise:
- Einfluss verschiedener Punkte evaluieren:
    - Beste variante nehmen und dann jeweils einen Punkt wegnehmen + evaluieren
        - Was bringt Indexing
        - Was bringen views
        - ...

## Fragen
  - Nur One Step reasoning oder vollständiges Reasoning oder beides? (Bisher nur one-step reasoning)
  - Laut AnyBURL Paper erkenntnissen sind nur kurze Regeln gut in Embedding Models repräsentiert
    - Sollen wir uns nur auf die Vergleichbarkeit mit AnyBURL und der Code Umsetzung fokussieren oder auch Messungen für größere Regelmengen machen?
  - Ist die Grundstuktur nachvollziehbar

